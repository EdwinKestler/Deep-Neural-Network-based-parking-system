{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parking.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankit1khare/Deep-Neural-Network-based-parking-system/blob/master/DeepParking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YniylcoaYBU3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R7mRiZpYYObm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LKEjnxdGZAbX",
        "colab_type": "code",
        "outputId": "918af581-a460-40c6-8556-f9092d277f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fH5NJYvYY2G0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/AK49/Parking/fastai/courses/dl1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuhkIQObZFS8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install fastai==0.7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E0BGlgUyev4-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "uZq3c5eWaq0F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install torchtext==0.2.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z26pU9tdetzf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The above version of Torch text is required with fastai library**"
      ]
    },
    {
      "metadata": {
        "id": "BEBv-0mZfDbZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r '/content/drive/My Drive/AK49/Parking/fastai/courses/dl1/data/tmp'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5s9Y-1UNftdn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Remove tmp folder to avoid errors**"
      ]
    },
    {
      "metadata": {
        "id": "Asc1syE3YBU-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import numpy as np\n",
        "import cv2\n",
        "# This file contains all the main external libs we'll use\n",
        "from fastai.imports import *\n",
        "from fastai.transforms import *\n",
        "from fastai.conv_learner import *\n",
        "from fastai.model import *\n",
        "from fastai.dataset import *\n",
        "from fastai.sgdr import *\n",
        "from fastai.plots import *\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "from PIL import ImageDraw, ImageFont\n",
        "from matplotlib import patches, patheffects\n",
        "\n",
        "cv2.__version__\n",
        "sys.version_info \n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tFakO2ExYBVB",
        "colab_type": "code",
        "outputId": "04b7be3b-02a6-43f5-8579-d62326df5645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# path references\n",
        "fn = \"Khare_testvideo_01.mp4\" #3\n",
        "#fn = \"datasets\\parkinglot_1_720p.mp4\"\n",
        "#fn = \"datasets\\street_high_360p.mp4\"\n",
        "fn_yaml = \"Khare_yml_01.yml\"\n",
        "fn_out =  \"Khare_outputvideo_01.avi\"\n",
        "# cascade_src = 'Khare_classifier_02.xml'\n",
        "# car_cascade = cv2.CascadeClassifier(cascade_src)\n",
        "global_str = \"Last change at: \"\n",
        "change_pos = 0.00\n",
        "dict =  {\n",
        "        'text_overlay': True,\n",
        "        'parking_overlay': True,\n",
        "        'parking_id_overlay': True,\n",
        "        'parking_detection': True,\n",
        "        'motion_detection': True,\n",
        "        'min_area_motion_contour': 500, # area given to detect motion\n",
        "        'park_laplacian_th': 2.8, \n",
        "        'park_sec_to_wait': 2, # 4   wait time for changing the status of a region\n",
        "        'start_frame': 0, # begin frame from specific frame number \n",
        "        'show_ids': True, # shows id on each region\n",
        "        'classifier_used': True,\n",
        "        'save_video': False\n",
        "        }\n",
        "\n",
        "# Set from video\n",
        "cap = cv2.VideoCapture(fn)\n",
        "video_info = {  'fps':    cap.get(cv2.CAP_PROP_FPS),\n",
        "                'width':  int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)*0.6),\n",
        "                'height': int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)*0.6),\n",
        "                'fourcc': cap.get(cv2.CAP_PROP_FOURCC),\n",
        "                'num_of_frames': int(cap.get(cv2.CAP_PROP_FRAME_COUNT))}\n",
        "\n",
        "cap.set(cv2.CAP_PROP_POS_FRAMES, dict['start_frame']) # jump to frame number specified\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "JwtkRSOiYBVL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "arch=resnet101\n",
        "PATH = \"./data/\"\n",
        "sz=224 \n",
        "data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, sz))\n",
        "learn = ConvLearner.pretrained(arch, data, precompute=False)\n",
        "trn_tfms, val_tfms = tfms_from_model(arch,sz) # get transformations\n",
        "\n",
        "def run_classifier(img):\n",
        "    im = val_tfms(img)\n",
        "    learn.precompute=False # We'll pass in a raw image, not activations\n",
        "    log_preds = learn.predict_array(im[None])\n",
        "    pred = np.argmax(log_preds, axis=1)\n",
        "    probs = np.exp(log_preds)\n",
        "    if pred == 0:\n",
        "        return False\n",
        "#     elif probs[0,1]>0.65:\n",
        "#         return 1\n",
        "    else:\n",
        "        return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m-VcEar0YBVN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the codec and create VideoWriter object\n",
        "if dict['save_video']:\n",
        "    fourcc = cv2.VideoWriter_fourcc('X','V','I','D') # options: ('P','I','M','1'), ('D','I','V','X'), ('M','J','P','G'), ('X','V','I','D')\n",
        "    out = cv2.VideoWriter(fn_out, -1, 25.0,(video_info['width'], video_info['height']))\n",
        "\n",
        "# # initialize the HOG descriptor/person detector. Take a lot of processing power.\n",
        "# if dict['pedestrian_detection']:\n",
        "#     hog = cv2.HOGDescriptor()\n",
        "#     hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
        "\n",
        "    # Use Background subtraction\n",
        "if dict['motion_detection']:\n",
        "    fgbg = cv2.createBackgroundSubtractorMOG2(history=300, varThreshold=16, detectShadows=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2z7mPxd7YBVQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read YAML data (parking space polygons)\n",
        "with open(fn_yaml, 'r') as stream:\n",
        "    parking_data = yaml.load(stream)\n",
        "\n",
        "parking_contours = []\n",
        "parking_bounding_rects = []\n",
        "parking_mask = []\n",
        "parking_data_motion = []\n",
        "init_delta = []\n",
        "\n",
        "if parking_data != None:\n",
        "    for park in parking_data:\n",
        "        points = np.array(park['points'])\n",
        "        rect = cv2.boundingRect(points)\n",
        "        points_shifted = points.copy()\n",
        "        points_shifted[:,0] = points[:,0] - rect[0] # shift contour to region of interest\n",
        "        points_shifted[:,1] = points[:,1] - rect[1]\n",
        "        parking_contours.append(points)\n",
        "        parking_bounding_rects.append(rect)\n",
        "        mask = cv2.drawContours(np.zeros((rect[3], rect[2]), dtype=np.uint8), [points_shifted], contourIdx=-1,\n",
        "                                    color=255, thickness=-1, lineType=cv2.LINE_8)\n",
        "        mask = mask==255\n",
        "        parking_mask.append(mask)\n",
        "#         print(mask)\n",
        "\n",
        "kernel_erode = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3)) # morphological kernel\n",
        "kernel_dilate = cv2.getStructuringElement(cv2.MORPH_RECT,(5,19))\n",
        "if parking_data != None:\n",
        "    parking_status = [False]*len(parking_data)\n",
        "    parking_buffer = [None]*len(parking_data)\n",
        "    init_delta = [0]*len(parking_data)\n",
        "# bw = ()\n",
        "\n",
        "\n",
        "def print_parkIDs(park, coor_points, frame_rev):\n",
        "    moments = cv2.moments(coor_points)\n",
        "    centroid = (int(moments['m10']/moments['m00'])-3, int(moments['m01']/moments['m00'])+3)\n",
        "    # putting numbers on marked regions\n",
        "    cv2.putText(frame_rev, str(park['id']), (centroid[0]+1, centroid[1]+1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
        "    cv2.putText(frame_rev, str(park['id']), (centroid[0]-1, centroid[1]-1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
        "    cv2.putText(frame_rev, str(park['id']), (centroid[0]+1, centroid[1]-1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
        "    cv2.putText(frame_rev, str(park['id']), (centroid[0]-1, centroid[1]+1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n",
        "    cv2.putText(frame_rev, str(park['id']), centroid, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UkopYggjYBVT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1085
        },
        "outputId": "f55dca27-c92e-42e7-8757-5adc0c31c354"
      },
      "cell_type": "code",
      "source": [
        "countwhile = 0\n",
        "while(cap.isOpened()):\n",
        "    video_cur_pos = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0 # Current position of the video file in seconds\n",
        "    video_cur_frame = cap.get(cv2.CAP_PROP_POS_FRAMES) # Index of the frame to be decoded/captured next\n",
        "    ret, frame_initial = cap.read()\n",
        "    if ret == True:\n",
        "        frame = cv2.resize(frame_initial, None, fx=0.6, fy=0.6)\n",
        "    if ret == False:\n",
        "        print(\"Video ended\")\n",
        "        break\n",
        "\n",
        "    # Background Subtraction\n",
        "    frame_blur = cv2.GaussianBlur(frame.copy(), (5,5), 3)\n",
        "    # frame_blur = frame_blur[150:1000, 100:1800]\n",
        "    frame_gray = cv2.cvtColor(frame_blur, cv2.COLOR_BGR2GRAY)\n",
        "    frame_out = frame.copy()\n",
        "\n",
        "    # Drawing the Overlay. Text overlay at the left corner of screen\n",
        "    if dict['text_overlay']:\n",
        "        str_on_frame = \"%d/%d\" % (video_cur_frame, video_info['num_of_frames'])\n",
        "        cv2.putText(frame_out, str_on_frame, (5,30), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.8, (0,255,255), 2, cv2.LINE_AA)\n",
        "        cv2.putText(frame_out,global_str + str(round(change_pos,2)) + 'sec', (5, 60), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "        \n",
        "        \n",
        "    # motion detection for all objects\n",
        "    if dict['motion_detection']:\n",
        "        fgmask = fgbg.apply(frame_blur)\n",
        "        bw = np.uint8(fgmask==255)*255\n",
        "        bw = cv2.erode(bw, kernel_erode, iterations=1)\n",
        "        bw = cv2.dilate(bw, kernel_dilate, iterations=1)\n",
        "        (_, cnts, _) = cv2.findContours(bw.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        # loop over the contours\n",
        "        for c in cnts:\n",
        "            if cv2.contourArea(c) < dict['min_area_motion_contour']:\n",
        "                continue\n",
        "            (x, y, w, h) = cv2.boundingRect(c)\n",
        "            cv2.rectangle(frame_out, (x, y), (x + w, y + h), (255, 0, 0), 3)\n",
        "\n",
        "            \n",
        "            \n",
        "    # detecting cars and vacant spaces with LAPLACIAN\n",
        "    if dict['parking_detection']:\n",
        "        if countwhile == 1:\n",
        "            for ind, park in enumerate(parking_data):\n",
        "                points = np.array(park['points'])\n",
        "                rect = parking_bounding_rects[ind]\n",
        "                roi_gray = frame_gray[rect[1]:(rect[1]+rect[3]), rect[0]:(rect[0]+rect[2])] # crop roi for faster calcluation\n",
        "                laplacian = cv2.Laplacian(roi_gray, cv2.CV_64F)\n",
        "                # cv2.imshow('oir', laplacian)\n",
        "                points[:,0] = points[:,0] - rect[0] # shift contour to roi\n",
        "                points[:,1] = points[:,1] - rect[1]\n",
        "                init_delta[ind] = np.mean(np.abs(laplacian * parking_mask[ind]))\n",
        "                print(parking_mask[ind], laplacian * parking_mask[ind])\n",
        "        for ind, park in enumerate(parking_data):\n",
        "            points = np.array(park['points'])\n",
        "            rect = parking_bounding_rects[ind]\n",
        "            roi_gray = frame_gray[rect[1]:(rect[1]+rect[3]), rect[0]:(rect[0]+rect[2])] # crop roi for faster calcluation\n",
        "\n",
        "            laplacian = cv2.Laplacian(roi_gray, cv2.CV_64F)\n",
        "            # cv2.imshow('oir', laplacian)\n",
        "            points[:,0] = points[:,0] - rect[0] # shift contour to roi\n",
        "            points[:,1] = points[:,1] - rect[1]\n",
        "            delta = np.mean(np.abs(laplacian * parking_mask[ind]))\n",
        "            # if(delta<2.5):\n",
        "                # print(\"ind, del\", ind, delta)\n",
        "            status = delta < init_delta[ind]\n",
        "            # If detected a change in parking status, save the current time\n",
        "            if status != parking_status[ind] and parking_buffer[ind]==None:\n",
        "                parking_buffer[ind] = video_cur_pos\n",
        "                change_pos = video_cur_pos\n",
        "            # If status is still different than the one saved and counter is open\n",
        "            elif status != parking_status[ind] and parking_buffer[ind]!=None:\n",
        "                if video_cur_pos - parking_buffer[ind] > dict['park_sec_to_wait']:\n",
        "                    parking_status[ind] = status\n",
        "                    parking_buffer[ind] = None\n",
        "            # If status is still same and counter is open\n",
        "            elif status == parking_status[ind] and parking_buffer[ind]!=None:\n",
        "                parking_buffer[ind] = None\n",
        "\n",
        "    # changing the color on the basis on status change occured in the above section and putting numbers on areas\n",
        "    if dict['parking_overlay']:\n",
        "        for ind, park in enumerate(parking_data):\n",
        "            points = np.array(park['points'])\n",
        "            if parking_status[ind]:\n",
        "#                 color = (0,255,0)\n",
        "                rect = parking_bounding_rects[ind]\n",
        "                roi_gray_ov = frame[rect[1]:(rect[1] + rect[3]),\n",
        "                               rect[0]:(rect[0] + rect[2])]  # crop roi for faster calcluation\n",
        "                res = run_classifier(roi_gray_ov)\n",
        "                if res:\n",
        "                    parking_data_motion.append(parking_data[ind])\n",
        "                    color = (0,255,0)\n",
        "            else:\n",
        "                color = (0,0,255)\n",
        "            \n",
        "            cv2.drawContours(frame_out, [points], contourIdx=-1,\n",
        "                                 color=color, thickness=2, lineType=cv2.LINE_8)\n",
        "            if dict['show_ids']:\n",
        "                    print_parkIDs(park, points, frame_out)\n",
        "            \n",
        "            \n",
        "    # write the output frames\n",
        "    if dict['save_video']:\n",
        "#         if video_cur_frame % 35 == 0: # take every 30 frames\n",
        "            out.write(frame_out)\n",
        "\n",
        "    # Display video\n",
        "    #cv2.imshow('frame', frame_out)\n",
        "\n",
        " #     if video_cur_frame < 256:\n",
        "    cv2.imwrite('./videos/frameset/%d.jpg' % video_cur_frame, frame_out)\n",
        "#     else:\n",
        "#       break\n",
        "    countwhile+=1\n",
        "  \n",
        "    k = cv2.waitKey(1)\n",
        "    if k == ord('q'):\n",
        "        break\n",
        "    elif k == ord('c'):\n",
        "        cv2.imwrite('%d.jpg' % video_cur_frame, frame_out)\n",
        "    elif k == ord('j'):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, video_cur_frame+1000) # jump 1000 frames\n",
        "    elif k == ord('u'):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, video_cur_frame + 500)  # jump 500 frames\n",
        "    if cv2.waitKey(33) == 27:\n",
        "        break\n",
        "\n",
        "cv2.waitKey(0)\n",
        "cap.release()\n",
        "if dict['save_video']: out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-d1114fb2df05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m                 roi_gray_ov = frame[rect[1]:(rect[1] + rect[3]),\n\u001b[1;32m     91\u001b[0m                                rect[0]:(rect[0] + rect[2])]  # crop roi for faster calcluation\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroi_gray_ov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mparking_data_motion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparking_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-880d8d67f055>\u001b[0m in \u001b[0;36mrun_classifier\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_tfms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;31m# We'll pass in a raw image, not activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlog_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mpredict_array\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0mhas\u001b[0m \u001b[0many\u001b[0m \u001b[0meffect\u001b[0m \u001b[0monly\u001b[0m \u001b[0mon\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0msuch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDropout\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mBatchNorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \"\"\"\n\u001b[0;32m--> 681\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \"\"\"\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "b3jldJL7eL9-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -l ./videos/frameset/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-llMwHDg6QX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !mkdir ./videos/frameset\n",
        "# !rm -r ./videos/frameset\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lsW13D5cvS5I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "ROOT_DIR = os.getcwd()\n",
        "VIDEO_DIR = os.path.join(ROOT_DIR, \"videos\")\n",
        "VIDEO_SAVE_DIR = os.path.join(VIDEO_DIR, \"frameset\")\n",
        "images = list(glob.iglob(os.path.join(VIDEO_SAVE_DIR, '*.*')))\n",
        "# Sort the images by integer index\n",
        "images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
        "\n",
        "outvid = os.path.join(VIDEO_DIR, \"out.mp4\")\n",
        "\n",
        "# print(images)\n",
        "# for image in images:\n",
        "#   img = cv2.imread(image)\n",
        "#   print(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ygTl5gGyfo_G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def make_video(outvid, images=images, fps=30, size=None,\n",
        "               is_color=True, format=\"FMP4\"):\n",
        "    \"\"\"\n",
        "    Create a video from a list of images.\n",
        " \n",
        "    @param      outvid      output video\n",
        "    @param      images      list of images to use in the video\n",
        "    @param      fps         frame per second\n",
        "    @param      size        size of each frame\n",
        "    @param      is_color    color\n",
        "    @param      format      see http://www.fourcc.org/codecs.php\n",
        "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
        " \n",
        "    The function relies on http://opencv-python-tutroals.readthedocs.org/en/latest/.\n",
        "    By default, the video will have the size of the first image.\n",
        "    It will resize every image to this size before adding them to the video.\n",
        "    \"\"\"\n",
        "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
        "    fourcc = VideoWriter_fourcc(*format)\n",
        "    vid = None\n",
        "    for image in images:\n",
        "        if not os.path.exists(image):\n",
        "            raise FileNotFoundError(image)\n",
        "        img = imread(image)\n",
        "        if vid is None:\n",
        "            if size is None:\n",
        "                size = img.shape[1], img.shape[0]\n",
        "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
        "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
        "            img = resize(img, size)\n",
        "        vid.write(img)\n",
        "    vid.release()\n",
        "    return vid\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ix40DWYdgFSW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Directory of images to run detection on\n",
        "make_video(outvid, images, fps=30)\n",
        "\n",
        "# !mv frameset/ ./videos/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MD8ZHpvjYBVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''Run this section locally because GPU is not required for creating boxes and imshow will fail in colab'''",
        "# press escape to finish doing real time boxing.\n",
        "# Program marks the polygons in the figure when it gets 4 double clicks\n",
        "import cv2\n",
        "import yaml\n",
        "import numpy as np\n",
        "\n",
        "refPt = []\n",
        "cropping = False\n",
        "data = []\n",
        "file_path = 'Khare_yml_02.yml'\n",
        "img = cv2.imread('Khare_testFrame_02.jpg')\n",
        "\n",
        "def yaml_loader(file_path):\n",
        "    with open(file_path, \"r\") as file_descr:\n",
        "        data = yaml.load(file_descr)\n",
        "        return data\n",
        "\n",
        "\n",
        "def yaml_dump(file_path, data):\n",
        "    with open(file_path, \"a\") as file_descr:\n",
        "        yaml.dump(data, file_descr)\n",
        "\n",
        "\n",
        "def yaml_dump_write(file_path, data):\n",
        "    with open(file_path, \"w\") as file_descr:\n",
        "        yaml.dump(data, file_descr)\n",
        "\n",
        "\n",
        "def click_and_crop(event, x, y, flags, param):\n",
        "    current_pt = {'id': 0, 'points': []}\n",
        "    # grab references to the global variables\n",
        "    global refPt, cropping\n",
        "    if event == cv2.EVENT_LBUTTONDBLCLK:\n",
        "        refPt.append((x, y))\n",
        "        cropping = False\n",
        "    if len(refPt) == 4:\n",
        "        if data == []:\n",
        "            if yaml_loader(file_path) != None:\n",
        "                data_already = len(yaml_loader(file_path))\n",
        "            else:\n",
        "                data_already = 0\n",
        "        else:\n",
        "            if yaml_loader(file_path) != None:\n",
        "                data_already = len(data) + len(yaml_loader(file_path))\n",
        "            else:\n",
        "                data_already = len(data) \n",
        "        \n",
        "        cv2.line(image, refPt[0], refPt[1], (0, 255, 0), 1)\n",
        "        cv2.line(image, refPt[1], refPt[2], (0, 255, 0), 1)\n",
        "        cv2.line(image, refPt[2], refPt[3], (0, 255, 0), 1)\n",
        "        cv2.line(image, refPt[3], refPt[0], (0, 255, 0), 1)\n",
        "\n",
        "        temp_lst1 = list(refPt[2])\n",
        "        temp_lst2 = list(refPt[3])\n",
        "        temp_lst3 = list(refPt[0])\n",
        "        temp_lst4 = list(refPt[1])\n",
        "\n",
        "        current_pt['points'] = [temp_lst1, temp_lst2, temp_lst3, temp_lst4]\n",
        "        current_pt['id'] = data_already\n",
        "        data.append(current_pt)\n",
        "        # data_already+=1\n",
        "        refPt = []\n",
        "image = cv2.resize(img, None, fx=0.6, fy=0.6)\n",
        "clone = image.copy()\n",
        "cv2.namedWindow(\"Double click to mark points\")\n",
        "cv2.imshow(\"Double click to mark points\", image)\n",
        "cv2.setMouseCallback(\"Double click to mark points\", click_and_crop)\n",
        "\n",
        "# keep looping until the 'q' key is pressed\n",
        "while True:\n",
        "    # display the image and wait for a keypress\n",
        "    cv2.imshow(\"Double click to mark points\", image)\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    if cv2.waitKey(33) == 27:\n",
        "        break\n",
        "       \n",
        "# data list into yaml file\n",
        "if data != []:\n",
        "    yaml_dump(file_path, data)\n",
        "cv2.destroyAllWindows() #important to prevent window from becoming inresponsive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zd5-UEc8YBVZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}